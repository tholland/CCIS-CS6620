{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (c) 2014 Reid Johnson\n",
    "#\n",
    "# Modified from:\n",
    "# (c) 2013 Mikael Vejdemo-Johansson\n",
    "# BSD License\n",
    "#\n",
    "# SciPy function to compute the gap statistic for evaluating k-means clustering.\n",
    "#\n",
    "# The gap statistic is defined by Tibshirani, Walther, Hastie in:\n",
    "#  Estimating the number of clusters in a data set via the gap statistic\n",
    "#  J. R. Statist. Soc. B (2001) 63, Part 2, pp 411-423\n",
    "\n",
    "import scipy as sp\n",
    "import scipy as sp\n",
    "import scipy.cluster.vq\n",
    "import scipy.spatial.distance\n",
    "import scipy.stats\n",
    "import sklearn.cluster\n",
    "\n",
    "import pylab as pl\n",
    "\n",
    "dst = sp.spatial.distance.euclidean\n",
    "\n",
    "def gap_statistics(data, refs=None, nrefs=20, ks=range(1,11)):\n",
    "    \"\"\"Computes the gap statistics for an nxm dataset.\n",
    "\n",
    "    The gap statistic measures the difference between within-cluster dispersion on an input\n",
    "    dataset and that expected under an appropriate reference null distribution.\n",
    "\n",
    "    Computation of the gap statistic, then, requires a series of reference (null) distributions.\n",
    "    One may either input a precomputed set of reference distributions (via the parameter refs)\n",
    "    or specify the number of reference distributions (via the parameter nrefs) for automatic \n",
    "    generation of uniform distributions within the bounding box of the dataset (data).\n",
    "\n",
    "    Each computation of the gap statistic requires the clustering of the input dataset and of\n",
    "    several reference distributions. To identify the optimal number of clusters k, the gap \n",
    "    statistic is computed over a range of possible values of k (via the parameter ks).\n",
    "\n",
    "    For each value of k, within-cluster dispersion is calculated for the input dataset and each\n",
    "    reference distribution. The calculation of the within-cluster dispersion for the reference\n",
    "    distributions will have a degree of variation, which we measure by standard deviation or\n",
    "    standard error.\n",
    "\n",
    "    The estimated optimal number of clusters, then, is defined as the smallest value k such that\n",
    "    gap_k is greater than or equal to the sum of gap_k+1 minus the expected error err_k+1.\n",
    "\n",
    "    Args:\n",
    "      data ((n,m) SciPy array): The dataset on which to compute the gap statistics.\n",
    "      refs ((n,m,k) SciPy array, optional): A precomputed set of reference distributions. \n",
    "        Defaults to None.\n",
    "      nrefs (int, optional): The number of reference distributions for automatic generation. \n",
    "        Defaults to 20.\n",
    "      ks (list, optional): The list of values k for which to compute the gap statistics. \n",
    "        Defaults to range(1,11), which creates a list of values from 1 to 10.\n",
    "\n",
    "    Returns:\n",
    "      gaps: an array of gap statistics computed for each k.\n",
    "      errs: an array of standard errors (se), with one corresponding to each gap computation.\n",
    "      difs: an array of differences between each gap_k and the sum of gap_k+1 minus err_k+1.\n",
    "\n",
    "    \"\"\"\n",
    "    shape = data.shape\n",
    "\n",
    "    if refs==None:\n",
    "        tops = data.max(axis=0) # maxima along the first axis (rows)\n",
    "        bots = data.min(axis=0) # minima along the first axis (rows)\n",
    "        dists = sp.matrix(sp.diag(tops-bots)) # the bounding box of the input dataset\n",
    "\n",
    "        # Generate nrefs uniform distributions each in the half-open interval [0.0, 1.0)\n",
    "        rands = sp.random.random_sample(size=(shape[0],shape[1], nrefs))\n",
    "\n",
    "        # Adjust each of the uniform distributions to the bounding box of the input dataset\n",
    "        for i in range(nrefs):\n",
    "            rands[:,:,i] = rands[:,:,i]*dists+bots\n",
    "    else:\n",
    "        rands = refs\n",
    "\n",
    "    gaps = sp.zeros((len(ks),))   # array for gap statistics (lenth ks)\n",
    "    errs = sp.zeros((len(ks),))   # array for model standard errors (length ks)\n",
    "    difs = sp.zeros((len(ks)-1,)) # array for differences between gaps (length ks-1)\n",
    "\n",
    "    for (i,k) in enumerate(ks): # iterate over the range of k values\n",
    "        # Cluster the input dataset via k-means clustering using the current value of k\n",
    "        try:\n",
    "            (kmc,kml) = sp.cluster.vq.kmeans2(data, k)\n",
    "        except LinAlgError:\n",
    "            kmeans = sklearn.cluster.KMeans(n_clusters=k).fit(data)\n",
    "            (kmc, kml) = kmeans.cluster_centers_, kmeans.labels_\n",
    "\n",
    "        # Generate within-dispersion measure for the clustering of the input dataset\n",
    "        disp = sum([dst(data[m,:],kmc[kml[m],:]) for m in range(shape[0])])\n",
    "\n",
    "        # Generate within-dispersion measures for the clusterings of the reference datasets\n",
    "        refdisps = sp.zeros((rands.shape[2],))\n",
    "        for j in range(rands.shape[2]):\n",
    "            # Cluster the reference dataset via k-means clustering using the current value of k\n",
    "            try:\n",
    "                (kmc,kml) = sp.cluster.vq.kmeans2(rands[:,:,j], k)\n",
    "            except LinAlgError:\n",
    "                kmeans = sklearn.cluster.KMeans(n_clusters=k).fit(rands[:,:,j])\n",
    "                (kmc, kml) = kmeans.cluster_centers_, kmeans.labels_\n",
    "\n",
    "            refdisps[j] = sum([dst(rands[m,:,j],kmc[kml[m],:]) for m in range(shape[0])])\n",
    "\n",
    "        # Compute the (estimated) gap statistic for k\n",
    "        gaps[i] = sp.mean(sp.log(refdisps) - sp.log(disp))\n",
    "\n",
    "        # Compute the expected error for k\n",
    "        errs[i] = sp.sqrt(sum(((sp.log(refdisp)-sp.mean(sp.log(refdisps)))**2) \\\n",
    "                              for refdisp in refdisps)/float(nrefs)) * sp.sqrt(1+1/nrefs)\n",
    "\n",
    "    # Compute the difference between gap_k and the sum of gap_k+1 minus err_k+1\n",
    "    difs = sp.array([gaps[k] - (gaps[k+1]-errs[k+1]) for k in range(len(gaps)-1)])\n",
    "\n",
    "    #print \"Gaps: \" + str(gaps)\n",
    "    #print \"Errs: \" + str(errs)\n",
    "    #print \"Difs: \" + str(difs)\n",
    "\n",
    "    return gaps, errs, difs\n",
    "\n",
    "def plot_gap_statistics(gaps, errs, difs):\n",
    "    \"\"\"Generates and shows plots for the gap statistics.\n",
    "\n",
    "    A figure with two subplots is generated. The first subplot is an errorbar plot of the \n",
    "    estimated gap statistics computed for each value of k. The second subplot is a barplot \n",
    "    of the differences in the computed gap statistics.\n",
    "\n",
    "    Args:\n",
    "      gaps (SciPy array): An array of gap statistics, one computed for each k.\n",
    "      errs (SciPy array): An array of standard errors (se), with one corresponding to each gap \n",
    "        computation.\n",
    "      difs (SciPy array): An array of differences between each gap_k and the sum of gap_k+1 \n",
    "        minus err_k+1.\n",
    "\n",
    "    \"\"\"\n",
    "    # Create a figure\n",
    "    fig = pl.figure(figsize=(16, 4))\n",
    "\n",
    "    pl.subplots_adjust(wspace=0.35) # adjust the distance between figures\n",
    "\n",
    "    # Subplot 1\n",
    "    ax = fig.add_subplot(121)\n",
    "    ind = range(1,len(gaps)+1) # the x values for the gaps\n",
    "\n",
    "    # Create an errorbar plot\n",
    "    rects = ax.errorbar(ind, gaps, yerr=errs, xerr=None, linewidth=1.0)\n",
    "\n",
    "    # Add figure labels and ticks\n",
    "    ax.set_title('Clustering Gap Statistics', fontsize=16)\n",
    "    ax.set_xlabel('Number of clusters k', fontsize=14)\n",
    "    ax.set_ylabel('Gap Statistic', fontsize=14)\n",
    "    ax.set_xticks(ind)\n",
    "\n",
    "    # Add figure bounds\n",
    "    ax.set_ylim(0, max(gaps+errs)*1.1)\n",
    "    ax.set_xlim(0, len(gaps)+1.0)\n",
    "\n",
    "    # Subplot 2\n",
    "    ax = fig.add_subplot(122)\n",
    "    ind = range(1,len(difs)+1) # the x values for the difs\n",
    "    \n",
    "    max_gap = None\n",
    "    if len(np.where(difs > 0)[0]) > 0:\n",
    "        max_gap = np.where(difs > 0)[0][0] + 1 # the k with the first positive dif\n",
    "\n",
    "    # Create a bar plot\n",
    "    ax.bar(ind, difs, alpha=0.5, color='g', align='center')\n",
    "\n",
    "    # Add figure labels and ticks\n",
    "    if max_gap:\n",
    "        ax.set_title('Clustering Gap Differences\\n(k=%d Estimated as Optimal)' % (max_gap), \\\n",
    "                     fontsize=16)\n",
    "    else:\n",
    "        ax.set_title('Clustering Gap Differences\\n', fontsize=16)\n",
    "    ax.set_xlabel('Number of clusters k', fontsize=14)\n",
    "    ax.set_ylabel('Gap Difference', fontsize=14)\n",
    "    ax.xaxis.set_ticks(range(1,len(difs)+1))\n",
    "\n",
    "    # Add figure bounds\n",
    "    ax.set_ylim(min(difs)*1.2, max(difs)*1.2)\n",
    "    ax.set_xlim(0, len(difs)+1.0)\n",
    "\n",
    "    # Show the figure\n",
    "    pl.show()\n",
    "\n",
    "# (c) 2014 Reid Johnson\n",
    "# BSD License\n",
    "#\n",
    "# Function to compute the sum of squared distance (SSQ) for evaluating k-means clustering.\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn.cluster\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "\n",
    "import pylab as pl\n",
    "\n",
    "def ssq_statistics(data, ks=range(1,11), ssq_norm=True):\n",
    "    \"\"\"Computes the sum of squares for an nxm dataset.\n",
    "\n",
    "    The sum of squares (SSQ) is a measure of within-cluster variation that measures the sum of \n",
    "    squared distances from cluster prototypes.\n",
    "\n",
    "    Each computation of the SSQ requires the clustering of the input dataset. To identify the \n",
    "    optimal number of clusters k, the SSQ is computed over a range of possible values of k \n",
    "    (via the parameter ks). For each value of k, within-cluster dispersion is calculated for the \n",
    "    input dataset.\n",
    "\n",
    "    The estimated optimal number of clusters, then, is defined as the value of k prior to an\n",
    "    \"elbow\" point in the plot of SSQ values.\n",
    "\n",
    "    Args:\n",
    "      data ((n,m) SciPy array): The dataset on which to compute the gap statistics.\n",
    "      ks (list, optional): The list of values k for which to compute the gap statistics. \n",
    "        Defaults to range(1,11), which creates a list of values from 1 to 10.\n",
    "\n",
    "    Returns:\n",
    "      ssqs: an array of SSQs, one computed for each k.\n",
    "\n",
    "    \"\"\"\n",
    "    ssqs = sp.zeros((len(ks),)) # array for SSQs (lenth ks)\n",
    "\n",
    "    #n_samples, n_features = data.shape # the number of rows (samples) and columns (features)\n",
    "    #if n_samples >= 2500:\n",
    "    #    # Generate a small sub-sample of the data\n",
    "    #    data_sample = shuffle(data, random_state=0)[:1000]\n",
    "    #else:\n",
    "    #    data_sample = data\n",
    "\n",
    "    for (i,k) in enumerate(ks): # iterate over the range of k values        \n",
    "        # Fit the model on the data\n",
    "        kmeans = sklearn.cluster.KMeans(n_clusters=k, random_state=0).fit(data)\n",
    "\n",
    "        # Predict on the data (k-means) and get labels\n",
    "        #labels = kmeans.predict(data)\n",
    "\n",
    "        if ssq_norm:\n",
    "            dist = np.min(cdist(data, kmeans.cluster_centers_, 'euclidean'), axis=1)\n",
    "\n",
    "            tot_withinss = sum(dist**2) # Total within-cluster sum of squares\n",
    "            totss = sum(pdist(data)**2) / data.shape[0] # The total sum of squares\n",
    "            betweenss = totss - tot_withinss # The between-cluster sum of squares\n",
    "            ssqs[i] = betweenss/totss*100\n",
    "        else:\n",
    "            # The sum of squared error (SSQ) for k\n",
    "            ssqs[i] = kmeans.inertia_\n",
    "\n",
    "    return ssqs\n",
    "\n",
    "def plot_ssq_statistics(ssqs):\n",
    "    \"\"\"Generates and shows plots for the sum of squares (SSQ).\n",
    "\n",
    "    A figure with one plot is generated. The plot is a bar plot of the SSQ computed for each \n",
    "    value of k.\n",
    "\n",
    "    Args:\n",
    "      ssqs (SciPy array): An array of SSQs, one computed for each k.\n",
    "\n",
    "    \"\"\"\n",
    "    # Create a figure\n",
    "    fig = pl.figure(figsize=(6.75, 4))\n",
    "\n",
    "    ind = range(1,len(ssqs)+1) # the x values for the ssqs\n",
    "    width = 0.5 # the width of the bars\n",
    "\n",
    "    # Create a bar plot\n",
    "    #rects = pl.bar(ind, ssqs, width)\n",
    "    pl.plot(ind, ssqs)\n",
    "\n",
    "    # Add figure labels and ticks\n",
    "    pl.title('Clustering Sum of Squared Distances', fontsize=16)\n",
    "    pl.xlabel('Number of clusters k', fontsize=14)\n",
    "    pl.ylabel('Sum of Squared Distance (SSQ)', fontsize=14)\n",
    "    pl.xticks(ind)\n",
    "\n",
    "    # Add text labels\n",
    "    #for rect in rects:\n",
    "    #    height = rect.get_height()\n",
    "    #    pl.text(rect.get_x()+rect.get_width()/2., 1.05*height, '%d' % int(height), \\\n",
    "    #            ha='center', va='bottom')\n",
    "\n",
    "    # Add figure bounds\n",
    "    pl.ylim(0, max(ssqs)*1.2)\n",
    "    pl.xlim(0, len(ssqs)+1.0)\n",
    "\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
